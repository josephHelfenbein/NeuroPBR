import Foundation
import CoreML
import UIKit
import Flutter

@available(iOS 16.0, *)
class PBRModelHandler {
    // MAIN FUNCTION CALLED BY FLUTTER
    func generatePBR(call: FlutterMethodCall, result: @escaping FlutterResult) {
        DispatchQueue.global(qos: .userInitiated).async {
            do {
                // 1. Parse arguments (raw bytes from Flutter)
                guard let args = call.arguments as? [String: Any],
                      let v1Data = (args["view1"] as? FlutterStandardTypedData)?.data,
                      let v2Data = (args["view2"] as? FlutterStandardTypedData)?.data,
                      let v3Data = (args["view3"] as? FlutterStandardTypedData)?.data else {
                    result(FlutterError(code: "INVALID_ARGS", message: "Missing image data", details: nil))
                    return
                }

                // 2. Load Core ML model
                // NOTE: The class 'pbr_model' is auto-generated by Xcode when you add pbr_model.mlpackage
                // Ensure you have added the model to the 'Runner' target.
                let config = MLModelConfiguration()
                config.computeUnits = .all // Use Neural Engine if available
                let model = try pbr_model(configuration: config)

                // 3. Preprocess images (Resize to 2048x2048 & convert to CVPixelBuffer)
                // The model expects inputs named "view1", "view2", "view3"
                guard let buffer1 = self.buffer(from: v1Data),
                      let buffer2 = self.buffer(from: v2Data),
                      let buffer3 = self.buffer(from: v3Data) else {
                    result(FlutterError(code: "IMAGE_PROC_ERROR", message: "Failed to process input images", details: nil))
                    return
                }

                // 4. Run Inference
                let prediction = try model.prediction(view1: buffer1, view2: buffer2, view3: buffer3)

                // 5. Extract outputs & convert to Data (PNG/Raw)
                // Model outputs: albedo, normal, roughness, metallic
                let albedoData = self.data(from: prediction.albedo)
                
                // Normal Map Handling:
                // The Core ML model is configured (via scale=0.5, bias=0.5) to map
                // the internal [-1, 1] normals to [0, 1] in the output image.
                // This produces a standard normal map texture (where 0.5 is 0).
                let normalData = self.data(from: prediction.normal)
                
                let roughnessData = self.data(from: prediction.roughness)
                let metallicData = self.data(from: prediction.metallic)

                // 6. Return to Flutter
                let response: [String: Any] = [
                    "albedo": albedoData ?? Data(),
                    "normal": normalData ?? Data(),
                    "roughness": roughnessData ?? Data(),
                    "metallic": metallicData ?? Data()
                ]
                
                DispatchQueue.main.async {
                    result(response)
                }

            } catch {
                DispatchQueue.main.async {
                    result(FlutterError(code: "INFERENCE_ERROR", message: error.localizedDescription, details: nil))
                }
            }
        }
    }

    // --- HELPER: Resizes and converts Data -> CVPixelBuffer ---
    private func buffer(from imageData: Data) -> CVPixelBuffer? {
        guard let image = UIImage(data: imageData),
        let cgImage = image.cgImage else { return nil }

        let options: [CFString: Any] = [
            kCVPixelBufferCGImageCompatibilityKey: true,
            kCVPixelBufferCGBitmapContextCompatibilityKey: true
        ]

        var pxBuffer: CVPixelBuffer?
        let status = CVPixelBufferCreate(kCFAllocatorDefault,
            2048, 2048, // MODEL SIZE
            kCVPixelFormatType_32ARGB,
            options as CFDictionary,
            &pxBuffer)

        guard status == kCVReturnSuccess, let buffer = pxBuffer else { return nil }

        CVPixelBufferLockBaseAddress(buffer, [])
        let context = CGContext(data: CVPixelBufferGetBaseAddress(buffer),
            width: 2048,
            height: 2048,
            bitsPerComponent: 8,
            bytesPerRow: CVPixelBufferGetBytesPerRow(buffer),
            space: CGColorSpaceCreateDeviceRGB(),
            bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue)

        // Draw the image into the context (This handles resizing automatically)
        context?.draw(cgImage, in: CGRect(x: 0, y: 0, width: 2048, height: 2048))
        CVPixelBufferUnlockBaseAddress(buffer, [])

        return buffer
    }

    // --- HELPER: CVPixelBuffer -> Data (PNG) ---
    private func data(from buffer: CVPixelBuffer) -> Data? {
        let ciImage = CIImage(cvPixelBuffer: buffer)
        let context = CIContext()
        if let cgImage = context.createCGImage(ciImage, from: ciImage.extent) {
            return UIImage(cgImage: cgImage).pngData()
        }
        return nil
    }
}
